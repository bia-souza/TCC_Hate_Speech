{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação por meio de um método de aprendizado de máquina\n",
    "\n",
    "Os embeddings podem oferecer uma informação de proximidade de conceitos que o uso de Bag of Words não seria capaz. Mesmo assim, cada representação e preprocessamento tem sua vantagem e desvantagem e não existe um método que será sempre o melhor. Assim, para sabermos qual representação é melhor para uma tarefa, é importante avaliarmos em quais delas são maiores para a tarefa em questão. Como o foco desta prática não é a avaliação, iremos apenas apresentar o resultado, caso queira, você pode [assistir a video aula](https://www.youtube.com/watch?v=Ag06UuWTsr4&list=PLwIaU1DGYV6tUx10fCTw5aPnqypbbK_GJ&index=12) e [fazer a prática sobre avaliação](https://github.com/daniel-hasan/ap-de-maquina-cefetmg-avaliacao/archive/master.zip). Nesta parte, iremos apenas usar a avaliação para verificar qual método é melhor.  \n",
    "\n",
    "Para que esta seção seja auto contida, iremos fazer toda a preparação que fizemos nas seções anteriores\n",
    "\n",
    "**Criação da lista de stopwords e de vocabulário:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000: distribuída\n",
      "20000: diferenciados\n",
      "30000: socialite\n",
      "40000: bárbaras\n",
      "50000: seguro-desemprego\n",
      "60000: interligada\n",
      "70000: landi\n",
      "80000: hurts\n",
      "90000: jackeline\n",
      "100000: cataluña\n",
      "110000: héber\n",
      "120000: calama\n",
      "130000: afogue\n",
      "140000: natalícios\n",
      "150000: amostrada\n",
      "160000: portageiros\n",
      "170000: ozias\n",
      "180000: banerjee\n",
      "190000: crackdown\n",
      "200000: kirchspielslandgemeinde\n",
      "210000: yello\n",
      "220000: picrodendraceae\n",
      "230000: rochlitz\n",
      "240000: illis\n",
      "250000: oitis\n",
      "260000: kalki\n",
      "270000: autorizagäo\n",
      "280000: goleminov\n",
      "290000: mamita\n",
      "300000: interessarmos\n",
      "310000: cprp\n",
      "320000: samitier\n",
      "330000: dimitre\n",
      "340000: montegranaro\n",
      "350000: sanguineti\n",
      "360000: wurmser\n",
      "370000: villaronga\n",
      "380000: zimbra\n",
      "390000: salvini-plawen\n",
      "400000: pankisi\n",
      "410000: hi-c\n",
      "420000: boggio\n",
      "430000: super-pena\n",
      "440000: imecc\n",
      "450000: adamascados\n",
      "Palavras ignoradas: 3\n"
     ]
    }
   ],
   "source": [
    "# reset para liberar memória\n",
    "%reset -f\n",
    "# fim do reset para liberar memória\n",
    "\n",
    "from embeddings.utils import get_embedding, KDTreeEmbedding\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "emotion_words = {\n",
    "                    \"Sexism\":{\"mulher\",\"homem\",\"cis\",\"trans\",\"feminista\",\"burra\",\"feia\",\"gorda\",\"puta\",\"menina\"},\n",
    "                    \"Body\":{\"gorda\", \"gordo\", \"magro\", \"magra\",\"magrela\",\"magrelo\",\"feio\",\"feia\",\"burra\",\"feminista\",\"mulher\"},\n",
    "                    \"Racism\":{\"negro\",\"negra\", \"preto\", \"preta\", \"racismo\", \"branco\",\"branca\",\"latino\",\"inverso\"},\n",
    "                    \"Ideology\":{\"esquerda\",\"direita\",\"esquerdopata\",\"esquerdomacho\",\"direitista\",\"minion\"},\n",
    "                    \"Homophobia\":{\"fufa\",\"sapatão\",\"gay\",\"viado\",\"hetero\",\"bicha\"},\n",
    "                    \"Origin\":{\"latino\", \"brasileiro\", \"europeu\", \"gringo\"},\n",
    "                    \"Religion\":{\"muçulmano\", \"deus\", \"allah\", \"crente\",\"cristão\",\"evangélico\"},\n",
    "                    \"Health\":{\"deficiente\", \"amputado\", \"amputa\",\"cadeirante\"},\n",
    "                    \"OtherLifestyle\":{\"vegano\",\"vegetariano\",\"hippie\",\"rock\",\"emo\",\"nerd\"},\n",
    "                    \"Migrants\":{\"gringo\", \"estrangeiro\", \"Brasil\", \"Portugal\", \"Angola\"},\n",
    "                    \"Ageing\":{\"velho\",\"velha\",\"incapaz\",\"senil\",\"idoso\",\"idosa\"}\n",
    "            }\n",
    "dict_embedding = get_embedding(\"glove.pt.100.txt\")\n",
    "\n",
    "kdtree_embedding = KDTreeEmbedding(dict_embedding, \"kdt_pt.p\")\n",
    "\n",
    "#obtem as stopwords\n",
    "stop_words = set()\n",
    "with open(\"datasets/stopwords.txt\") as stop_file:\n",
    "    stop_words = set(stop_word[:-1] for stop_word in stop_file)\n",
    "\n",
    "\n",
    "#palavras chaves a serem consideradas\n",
    "set_vocabulary = set()\n",
    "for key_word, arr_related_words in emotion_words.items():\n",
    "    set_vocabulary.add(key_word)\n",
    "    set_vocabulary = set_vocabulary | set(arr_related_words)\n",
    "\n",
    "#kdtree - para gerar o conjunto com palavras chaves e suas similares\n",
    "vocabulary_expanded = []\n",
    "for word in set_vocabulary:\n",
    "    _, words = kdtree_embedding.get_most_similar_embedding(word,60)\n",
    "    vocabulary_expanded.extend(words)\n",
    "vocabulary_expanded = set(vocabulary_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Representações usadas**:Iremos avaliar a filtragem de stopwords e usando um vocabulário restrito da representação bag of words e também da representação usando a média de embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings.textual_representation import BagOfWords, AggregateEmbeddings,InstanceWisePreprocess\n",
    "\n",
    "#gera as representações\n",
    "aggregate = AggregateEmbeddings(dict_embedding, \"avg\")\n",
    "embedding = InstanceWisePreprocess(\"embbeding\",aggregate)\n",
    "\n",
    "aggregate_stop = AggregateEmbeddings(dict_embedding, \"avg\",words_to_filter=stop_words)\n",
    "emb_nostop = InstanceWisePreprocess(\"emb_nostop\",aggregate_stop)\n",
    "\n",
    "\n",
    "aggregate_keywords_exp = AggregateEmbeddings(dict_embedding, \"avg\",words_to_consider=vocabulary_expanded)\n",
    "emb_keywords_exp = InstanceWisePreprocess(\"emb_keywords_exp\",aggregate_keywords_exp)\n",
    "\n",
    "bow_keywords = BagOfWords(\"bow_keywords_exp\", words_to_consider=vocabulary_expanded)\n",
    "bow = BagOfWords(\"bow\", stop_words=stop_words)\n",
    "\n",
    "arr_representations = [bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_hate_speech = pd.read_csv(\"2021-07-20_portuguese_hate_speech_hierarchical_classification.csv\",delimiter=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, é executado um método de aprendizado  para cada representação. Esse processo pode demorar um pouco pois é feito a procura do melhor parametro do algoritmo. Algumas otimizações que talvez, você precise fazer é no arquivo `embedding/avaliacao_embedding.py` alterar o parametro `n_jobs` no método `obtem_metodo` da classe `OtimizacaoObjetivoRandomForest`. Esse parametro é responsável por utiizar mais threads ao executar o Random Forests.  O valor pode ser levemente inferior a quantidades de núcleos que seu computador tem, caso ele tenha mais de 2, caso contrário, o ideal é colocarmos `n_jobs=1`. Caso queira visualizar resultados mais rapidamente, diminua o valor da variável `num_trials` e `num_folds` abaixo. Atenção que `num_folds` deve ser um valor maior que um."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Representação: bow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-21 00:41:46,235]\u001b[0m A new study created in RDB with name: Sexism_random_forest_bow_fold_0\u001b[0m\n",
      "\u001b[32m[I 2022-01-21 00:41:51,690]\u001b[0m Trial 0 finished with value: 0.8795503548320895 and parameters: {'min_samples_split': 9, 'max_features': 95, 'num_arvores': 30}. Best is trial 0 with value: 0.8795503548320895.\u001b[0m\n",
      "\u001b[32m[I 2022-01-21 00:41:56,127]\u001b[0m Trial 1 finished with value: 0.8828575123940281 and parameters: {'min_samples_split': 7, 'max_features': 75, 'num_arvores': 30}. Best is trial 1 with value: 0.8828575123940281.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8810480349344978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-21 00:41:59,678]\u001b[0m A new study created in RDB with name: Sexism_random_forest_bow_fold_1\u001b[0m\n",
      "\u001b[32m[I 2022-01-21 00:42:06,478]\u001b[0m Trial 0 finished with value: 0.8907007115892286 and parameters: {'min_samples_split': 5, 'max_features': 80, 'num_arvores': 35}. Best is trial 0 with value: 0.8907007115892286.\u001b[0m\n",
      "\u001b[32m[I 2022-01-21 00:42:12,369]\u001b[0m Trial 1 finished with value: 0.8861134996603363 and parameters: {'min_samples_split': 11, 'max_features': 80, 'num_arvores': 45}. Best is trial 0 with value: 0.8907007115892286.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8677772600186393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-21 00:42:16,476]\u001b[0m A new study created in RDB with name: Sexism_random_forest_bow_fold_2\u001b[0m\n",
      "\u001b[32m[I 2022-01-21 00:42:22,185]\u001b[0m Trial 0 finished with value: 0.8864228924416525 and parameters: {'min_samples_split': 5, 'max_features': 100, 'num_arvores': 30}. Best is trial 0 with value: 0.8864228924416525.\u001b[0m\n",
      "\u001b[32m[I 2022-01-21 00:42:27,375]\u001b[0m Trial 1 finished with value: 0.8830922465399595 and parameters: {'min_samples_split': 15, 'max_features': 80, 'num_arvores': 40}. Best is trial 0 with value: 0.8864228924416525.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8942218080149116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-21 00:42:31,518]\u001b[0m A new study created in RDB with name: Sexism_random_forest_bow_fold_3\u001b[0m\n",
      "\u001b[32m[I 2022-01-21 00:42:37,979]\u001b[0m Trial 0 finished with value: 0.8843338735304315 and parameters: {'min_samples_split': 3, 'max_features': 75, 'num_arvores': 50}. Best is trial 0 with value: 0.8843338735304315.\u001b[0m\n",
      "\u001b[32m[I 2022-01-21 00:42:44,059]\u001b[0m Trial 1 finished with value: 0.8799373783349146 and parameters: {'min_samples_split': 21, 'max_features': 80, 'num_arvores': 45}. Best is trial 0 with value: 0.8843338735304315.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9023999374609131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-21 00:42:48,864]\u001b[0m A new study created in RDB with name: Sexism_random_forest_bow_fold_4\u001b[0m\n",
      "\u001b[32m[I 2022-01-21 00:42:53,984]\u001b[0m Trial 0 finished with value: 0.8776282879488054 and parameters: {'min_samples_split': 19, 'max_features': 100, 'num_arvores': 30}. Best is trial 0 with value: 0.8776282879488054.\u001b[0m\n",
      "\u001b[32m[I 2022-01-21 00:42:59,845]\u001b[0m Trial 1 finished with value: 0.8830895380943135 and parameters: {'min_samples_split': 1, 'max_features': 75, 'num_arvores': 50}. Best is trial 1 with value: 0.8830895380943135.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9076798488325011\n",
      "Representação: bow Sexism concluida\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import optuna\n",
    "from embeddings.avaliacao_embedding import calcula_experimento_representacao, OtimizacaoObjetivoRandomForest\n",
    "\n",
    "# Método de aprendizado de máquina a ser usado\n",
    "dict_metodo = {\"random_forest\":{\"classe_otimizacao\":OtimizacaoObjetivoRandomForest,\n",
    "                                \"sampler\":optuna.samplers.TPESampler(seed=1, n_startup_trials=10)},\n",
    "              }\n",
    "df_amazon_reviews = pd.read_csv(\"merge.csv\",delimiter=\";\")\n",
    "\n",
    "#executa experimento com a representacao determinada e o método\n",
    "for metodo, param_metodo in dict_metodo.items():\n",
    "    for representation in arr_representations:\n",
    "        print(f\"===== Representação: {representation.nome}\")\n",
    "        #col_classe = [\"Sexism\"]\n",
    "        col_classe = [\"Sexism\",\"Racism\",\"Homophobia\",\"Body\"]\n",
    "        num_folds = 5\n",
    "        num_folds_validacao = 3\n",
    "        num_trials = 2\n",
    "\n",
    "        for col in col_classe:\n",
    "            nom_experimento = f\"{col}_{metodo}_\"+representation.nome\n",
    "            experimento = calcula_experimento_representacao(nom_experimento,representation,df_amazon_reviews,\n",
    "                                                col,num_folds,num_folds_validacao,num_trials,\n",
    "                                                ClasseObjetivoOtimizacao=param_metodo['classe_otimizacao'],\n",
    "                                                    sampler=param_metodo['sampler'])\n",
    "            print(f\"Representação: {representation.nome} {col} concluida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como a experimentação é uma tarefa custosa, todos os resultados são salvos na pasta \"resultados\" - inclusive os valores dos parametros na classe optuna (a prática de avaliação apresenta mais detalhes da biblioteca Optuna). A macro f1 é uma métrica relacionada a taxa de acerto (se necessário, [veja a explicação neste video - tópico 2 e 3)](https://www.youtube.com/watch?v=u7o7CSeXaNs&list=PLwIaU1DGYV6tUx10fCTw5aPnqypbbK_GJ&index=13). Analise os resultados abaixo: qual representação foi melhor? A restrição de vocabulário ou eliminação de stopwords auxiliou? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-21 00:43:08,503]\u001b[0m Using an existing study with name 'random_forest_bow_fold_0' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2022-01-21 00:43:09,373]\u001b[0m Using an existing study with name 'random_forest_bow_fold_1' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2022-01-21 00:43:10,218]\u001b[0m Using an existing study with name 'random_forest_bow_fold_2' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2022-01-21 00:43:11,120]\u001b[0m Using an existing study with name 'Sexism_random_forest_bow_fold_0' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2022-01-21 00:43:11,986]\u001b[0m Using an existing study with name 'Sexism_random_forest_bow_fold_1' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2022-01-21 00:43:12,883]\u001b[0m Using an existing study with name 'Sexism_random_forest_bow_fold_2' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2022-01-21 00:43:13,685]\u001b[0m Using an existing study with name 'Sexism_random_forest_bow_fold_3' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2022-01-21 00:43:14,529]\u001b[0m Using an existing study with name 'Sexism_random_forest_bow_fold_4' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2022-01-21 00:43:15,383]\u001b[0m Using an existing study with name 'random_forest_embbeding_fold_0' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2022-01-21 00:43:16,321]\u001b[0m Using an existing study with name 'random_forest_embbeding_fold_1' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2022-01-21 00:43:17,181]\u001b[0m Using an existing study with name 'random_forest_embbeding_fold_2' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2022-01-21 00:43:18,042]\u001b[0m Using an existing study with name 'random_forest_emb_keywords_exp_fold_0' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2022-01-21 00:43:18,878]\u001b[0m Using an existing study with name 'random_forest_emb_keywords_exp_fold_1' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2022-01-21 00:43:19,732]\u001b[0m Using an existing study with name 'random_forest_emb_keywords_exp_fold_2' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2022-01-21 00:43:20,594]\u001b[0m Using an existing study with name 'random_forest_bow_keywords_exp_fold_0' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2022-01-21 00:43:21,442]\u001b[0m Using an existing study with name 'random_forest_bow_keywords_exp_fold_1' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2022-01-21 00:43:22,323]\u001b[0m Using an existing study with name 'random_forest_bow_keywords_exp_fold_2' instead of creating a new one.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nom_experimento</th>\n",
       "      <th>macro-f1</th>\n",
       "      <th>f1-0</th>\n",
       "      <th>precision-0</th>\n",
       "      <th>recall-0</th>\n",
       "      <th>f1-1</th>\n",
       "      <th>precision-1</th>\n",
       "      <th>recall-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sexism_random_forest_bow</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.893057</td>\n",
       "      <td>0.901367</td>\n",
       "      <td>0.886019</td>\n",
       "      <td>0.888194</td>\n",
       "      <td>0.880360</td>\n",
       "      <td>0.897488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>random_forest_bow</td>\n",
       "      <td>0.792263</td>\n",
       "      <td>0.923971</td>\n",
       "      <td>0.887215</td>\n",
       "      <td>0.963920</td>\n",
       "      <td>0.660555</td>\n",
       "      <td>0.810754</td>\n",
       "      <td>0.557481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>random_forest_emb_keywords_exp</td>\n",
       "      <td>0.703635</td>\n",
       "      <td>0.905296</td>\n",
       "      <td>0.847843</td>\n",
       "      <td>0.971156</td>\n",
       "      <td>0.501973</td>\n",
       "      <td>0.779643</td>\n",
       "      <td>0.370442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest_embbeding</td>\n",
       "      <td>0.643717</td>\n",
       "      <td>0.892070</td>\n",
       "      <td>0.828357</td>\n",
       "      <td>0.966415</td>\n",
       "      <td>0.395363</td>\n",
       "      <td>0.695559</td>\n",
       "      <td>0.276213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>random_forest_bow_keywords_exp</td>\n",
       "      <td>0.623100</td>\n",
       "      <td>0.868929</td>\n",
       "      <td>0.826350</td>\n",
       "      <td>0.916186</td>\n",
       "      <td>0.377271</td>\n",
       "      <td>0.499625</td>\n",
       "      <td>0.303617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  nom_experimento  macro-f1      f1-0  precision-0  recall-0  \\\n",
       "1        Sexism_random_forest_bow  0.890625  0.893057     0.901367  0.886019   \n",
       "0               random_forest_bow  0.792263  0.923971     0.887215  0.963920   \n",
       "3  random_forest_emb_keywords_exp  0.703635  0.905296     0.847843  0.971156   \n",
       "2         random_forest_embbeding  0.643717  0.892070     0.828357  0.966415   \n",
       "4  random_forest_bow_keywords_exp  0.623100  0.868929     0.826350  0.916186   \n",
       "\n",
       "       f1-1  precision-1  recall-1  \n",
       "1  0.888194     0.880360  0.897488  \n",
       "0  0.660555     0.810754  0.557481  \n",
       "3  0.501973     0.779643  0.370442  \n",
       "2  0.395363     0.695559  0.276213  \n",
       "4  0.377271     0.499625  0.303617  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from base_am.avaliacao import Experimento\n",
    "\n",
    "arr_resultado = []\n",
    "for resultado_csv in os.listdir(\"resultados\"):\n",
    "    if resultado_csv.endswith(\"csv\"):\n",
    "        nom_experimento = resultado_csv.split(\".\")[0]\n",
    "        \n",
    "        #carrega resultados previamente realizados\n",
    "        experimento = Experimento(nom_experimento,[])\n",
    "        experimento.carrega_resultados_existentes()\n",
    "        \n",
    "        #adiciona experimento\n",
    "        num_folds = len(experimento.resultados)\n",
    "        dict_resultados = {\"nom_experimento\":nom_experimento, \n",
    "                            \"macro-f1\":sum([r.macro_f1 for r in experimento.resultados])/num_folds}\n",
    "        #resultados por classe\n",
    "        for classe in experimento.resultados[0].mat_confusao.keys():\n",
    "\n",
    "            dict_resultados[f\"f1-{classe}\"] = sum([r.f1_por_classe[classe] for r in experimento.resultados])/num_folds\n",
    "            dict_resultados[f\"precision-{classe}\"] = sum([r.precisao[classe] for r in experimento.resultados])/num_folds\n",
    "            dict_resultados[f\"recall-{classe}\"] = sum([r.revocacao[classe] for r in experimento.resultados])/num_folds\n",
    "\n",
    "        arr_resultado.append(dict_resultados)\n",
    "\n",
    "resultado = pd.DataFrame.from_dict(arr_resultado)\n",
    "resultado.sort_values(['macro-f1'], ascending = False, inplace = True)\n",
    "resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussão\n",
    "\n",
    "Estudando o resultado acima, ordenado em forma decrescente pelo critério macro $F1$ que leva em consideração as predições corretas, os falsos negativos e os falsos positivos do classificador para calcular o quão bom ele é em classificar os individuos de determinados grupos (quanto mais próximo de 1 esse valor for, melhor o classificador)\n",
    "\n",
    "Assim, verificamos que a representação bag of words é a que apresenta o maior macro $F1$, ou seja, a que mais analisa corretamente os sentimentos das revisões dos usuários. Porém, como dito anteriormente, essa representação é sujeita a uma limitação de generalização, visto que ela não permite calcular a distância entre palavras e consequentemente, a construir estruturas de sinônimos ou analogias por exemplo.\n",
    "\n",
    "Por fim, random_forest_bow é seguida pela random forest embedding e random forest embedding que desconsidera stopwords. Considerei isso um aspecto interessante, pois, stopwords em tese são palavras sem grande contribuição semântica para a extração do sentimento de sentenças, mas no caso presente, elas dão um incremento significativo (aproximadamente 2%) no macro $F1$ do classificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bolukbasi, T., Chang, K. W., Zou, J., Saligrama, V., & Kalai, A. (2016). **[Man is to computer programmer as woman is to homemaker? Debiasing word embeddings](https://arxiv.org/abs/1607.06520)**. \n",
    "\n",
    "Hartmann, N., Fonseca, E., Shulby, C., Treviso, M., Rodrigues, J., & Aluisio, S. (2017). [**Portuguese word embeddings: Evaluating on word analogies and natural language tasks.**](https://arxiv.org/abs/1708.06025)\n",
    "\n",
    "\n",
    "Pennington, J., Socher, R., & Manning, C. D. (2014, October).**[GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)**. In EMNLP 2015 \n",
    "\n",
    "\n",
    "Scherer, Klaus R. **[What are emotions? And how can they be measured?](https://journals.sagepub.com/doi/pdf/10.1177/0539018405058216)**. Social science information, v. 44, n. 4, p. 695-729, 2005.\n",
    "\n",
    "Shen, D., Wang, G., Wang, W., Min, M. R., Su, Q., Zhang, Y., Carin, L. (2018). [Baseline needs more love: On simple word-embedding-based models and associated pooling mechanisms](https://arxiv.org/pdf/1805.09843.pdf).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Licença Creative Commons\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br />Este obra está licenciado com uma Licença <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Atribuição-CompartilhaIgual 4.0 Internacional</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
